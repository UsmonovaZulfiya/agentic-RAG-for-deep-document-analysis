# üß† Agentic RAG for Deep Document Analysis
*Reproducibility of ‚ÄúBuilding Three Pipelines to Select the Right LLMs for RAG, Multi-Agent Systems, and Vision‚Äù*

---

## üìñ Overview

This repository reproduces and extends the experiment described in [Fareed Khan‚Äôs LevelUp article](https://levelup.gitconnected.com/building-three-pipelines-to-select-the-right-llms-for-rag-multi-agent-systems-and-vision-3e47e0563b76).  
The project implements an **Agentic Retrieval-Augmented Generation (RAG)** pipeline that uses **hierarchical chunking**, **multi-step reasoning**, and **model routing** to enable deep question answering over complex documents.

The pipeline simulates how autonomous agents can navigate large legal or technical documents ‚Äî selecting relevant text, refining search depth, and synthesizing structured answers with citations.

---

## ‚öôÔ∏è Core Features

- **Hierarchical Chunking** ‚Äì recursively splits documents into semantically meaningful segments using sentence-level boundaries.  
- **Agentic Navigation** ‚Äì a router model iteratively selects the most relevant text chunks based on a question.  
- **Answer Synthesis** ‚Äì a large instruction-tuned model summarizes retrieved evidence into a concise, citation-rich answer.  
- **Metrics Tracking** ‚Äì logs latency, token counts, and step-level performance for each reasoning stage.  
- **Reproducible Environment** ‚Äì compatible with [Nebius AI Studio](https://studio.nebius.ai) for hosted LLM inference.

---

## üß© Pipeline Architecture

1. **Document Loading** ‚Äì Fetch a PDF (e.g., the *Trademark Trial and Appeal Board Manual of Procedure*, TBMP) and extract its text.  
2. **Hierarchical Chunking** ‚Äì Divide the text into multi-level chunks for scalable navigation.  
3. **Routing & Reasoning** ‚Äì Use a smaller, efficient model (e.g., `meta-llama/Meta-Llama-3.1-8B-Instruct`) to reason about which chunks are most relevant.  
4. **Recursive Exploration** ‚Äì Subdivide selected chunks and re-evaluate until the maximum depth is reached.  
5. **Answer Synthesis** ‚Äì A high-capacity model (e.g., `meta-llama/Llama-3.3-70B-Instruct`) generates a structured answer with citations.  
6. **Evaluation (optional)** ‚Äì A separate model (e.g., `deepseek-ai/DeepSeek-V3`) can assess answer quality.

---

## üß∞ Technology Stack

| Component | Purpose | Example Model / Library |
|---|---|---|
| **Nebius AI Studio** | Hosted LLM inference endpoint | Llama-3.1 / Llama-3.3 / DeepSeek-V3 |
| **Hugging Face Transformers** | Tokenization and text preprocessing | `AutoTokenizer` |
| **PyPDF** | PDF text extraction | `pypdf.PdfReader` |
| **NLTK** | Sentence tokenization | `sent_tokenize` |
| **TQDM** | Progress monitoring | `tqdm.auto` |
| **OpenAI Python SDK** | Unified API interface | `openai.OpenAI()` |

---

## üß™ Reproducibility Steps

### 1) Clone the repository

```bash
git clone https://github.com/<your-username>/agentic-rag-reproduction.git
cd agentic-rag-reproduction
```

### 2) Install dependencies

```bash
pip install -r requirements.txt
```

Or install manually:

```bash
pip install openai pypdf nltk tqdm transformers pandas requests
```

### 3) Run the script

```bash
python agentic_rag_for_question_answering.py
```

Or open the notebook version in **Google Colab**.

### 4) Set up credentials

- Get your **Nebius AI API key** from <https://studio.nebius.ai>.  
- Log in to **Hugging Face** if using gated models:

```bash
huggingface-cli login
```

### 5) Observe the output

- The script prints chunking statistics, selected paragraphs, and a structured JSON answer with citations.  
- Metrics (latency, token counts, etc.) are stored in `metrics_log`.

---

## üìà Example Output

```text
--- GENERATED ANSWER ---
"The TBMP requires that a motion to compel discovery include a signed certificate of service,
specify the discovery requests at issue, and comply with standard formatting (ID: 1.3.5, 1.3.9)."

--- CITATIONS ---
['1.3.5', '1.3.9']
```

---

## üìÇ Repository Structure

```text
‚îú‚îÄ‚îÄ agentic_rag_for_question_answering.py   # Full reproducible script
‚îú‚îÄ‚îÄ untitled6.py                            # Cleaned-up variant for testing
‚îú‚îÄ‚îÄ requirements.txt                        # Dependency list
‚îú‚îÄ‚îÄ README.md                               # Project overview (this file)
‚îî‚îÄ‚îÄ /outputs/                               # Optional: logs and generated results
```

---

## üìö Research Context

This project illustrates how **agentic reasoning** and **RAG** pipelines can be combined to:

- Improve interpretability of multi-step LLM reasoning.  
- Emulate cognitive search behaviors in autonomous agents.  
- Enable scalable document analysis in domains like law, policy, or science.

It can serve as a foundation for future work on:

- Multi-agent retrieval coordination  
- RAG evaluation benchmarks  
- Adaptive routing and answer aggregation strategies  

---

## üî¨ Citation

If you use this code in your own research, please cite the original source:

> Fareed Khan, *‚ÄúBuilding Three Pipelines to Select the Right LLMs for RAG, Multi-Agent Systems, and Vision‚Äù*,  
> LevelUp, 2024. <https://levelup.gitconnected.com/building-three-pipelines-to-select-the-right-llms-for-rag-multi-agent-systems-and-vision-3e47e0563b76>

---

## üí≠ Author Notes

This repository was created as part of a **reproducibility study** on RAG pipelines and agentic LLM architectures.  
It aims to provide both an educational resource and a stepping stone for deeper experiments in **multi-agent retrieval**, **LLM benchmarking**, and **explainable AI**.

---

## ü™¥ Future Extensions

- Add multi-document retrieval and memory pooling  
- Integrate vector-based chunk selection  
- Evaluate answer consistency using a critic LLM  
- Visualize reasoning tree for transparency  

